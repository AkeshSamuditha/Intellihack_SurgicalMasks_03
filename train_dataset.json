{
    "queries": {
        "b2c84ac1-85e6-4ad0-8004-c6243b98a598": "**Question 1:**",
        "8b73ec09-a366-4296-837d-a13f14afb3cb": "Describe the core innovation of the DualPipe algorithm and how it addresses the challenges of pipeline bubbles in bidirectional pipeline parallelism.  Compare its performance in terms of bubble reduction and memory usage to the 1F1B and ZB1P methods, explaining the notation used in the table (F, B, W, F&B).",
        "e958aa00-60ed-4f08-b75d-d856301587bf": "Describe the two load balancing policies used in the DeepSeek-V3's `eplb.py` algorithm, specifying the scenarios in which each policy is most appropriate and the key differences in their approaches to expert replication and placement.",
        "4031759d-e218-4d6c-9218-9e3abc7c04e1": "Compare and contrast the performance of the Fire-Flyer File System (3FS) as demonstrated by the read stress test and the GraySort benchmark.  What aspects of the system architecture contribute to the observed performance in each case?",
        "ed04d362-a62b-4b76-ad2d-828c48b5eecc": "Describe the cluster architecture used for the sorting task, specifying the number and configuration of both storage and compute nodes, including relevant hardware specifications (e.g., NIC speeds, RAM).  What was the total data size sorted and the time taken to complete the task?",
        "de23b34d-1282-4374-988c-5fcfcaccce7c": "Explain the purpose of KVCache in the context of LLM inference.  What performance metrics related to KVCache are presented in the provided text, and what do these metrics represent (e.g., GiB/s, IOPS)?",
        "154dfc8b-e2af-42b2-a110-81a7a2e5b351": "Explain the architecture of the 3FS system, detailing the roles of each component and their interconnections.  Specifically address how the system handles failure of the primary cluster manager and the rationale behind using the same key-value store for both file metadata and cluster configuration.",
        "2bdfe314-6f3c-4e04-9aec-3a20bb59caf7": "Compare and contrast the FUSE and native clients for 3FS. Discuss the performance limitations of the FUSE client, focusing on the memory copy overhead and multi-threading limitations.  Explain why these limitations are particularly problematic for specific application scenarios described in the document (e.g., random access reads, concurrent writes).",
        "d661f6ad-890e-43c9-b823-827d20d3ff64": "Explain the design choices behind the asynchronous zero-copy API implemented within the FUSE daemon, including the roles of the `Iov` and `Ior` data structures and the rationale for using multiple Ior rings in multi-threaded applications.  Discuss the advantages and disadvantages of this approach compared to implementing the file system client as a VFS kernel module.",
        "71f20926-37d3-4e90-96b2-abdda2fb23bb": "Describe the 3FS file chunk allocation strategy, including how chunk IDs are generated and how data is distributed across replication chains.  Explain how this strategy contributes to balanced data distribution and the role of the metadata service in supporting file operations.  How does the use of FoundationDB as a metadata store enhance the system's scalability and resilience?",
        "4ccf1d63-24ea-4626-89cd-dbba5098a6ef": "Explain the mechanism 3FS uses to handle concurrent updates to a file's length, highlighting the role of the rendezvous hash algorithm and the potential for transaction conflicts.  Discuss the trade-offs involved in this approach.",
        "11821d12-fcf7-45ac-9525-e9588835e0a4": "Describe how 3FS manages file metadata, focusing on the data structures used (inodes and directory entries), their key components, and how they are stored and accessed using FoundationDB.  Explain how this design supports efficient directory listing and prevents directory loops.",
        "1c3fa216-3700-4b04-9e6b-8fb741e6028a": "Explain the mechanism used to mitigate concurrent file length updates in the meta service, including the algorithm employed and the rationale behind its use.  Discuss potential consequences if this mechanism were not in place.",
        "0bb70dd8-3956-45a7-a319-56ef6bde0056": "Describe the Chain Replication with Apportioned Queries (CRAQ) system, detailing how data is placed and accessed.  Illustrate your answer with a concrete example using the provided 6-node scenario, explaining how read and write operations are handled and the benefits of this approach in terms of scalability and fault tolerance.",
        "4e9b97f1-07cd-4205-a88c-3e38aae461d0": "Explain the purpose of multiple chain tables in the described storage system, providing a specific example of how different tables might be used to optimize for varying workload characteristics (e.g., batch vs. online processing).  How does the metadata service utilize these tables?",
        "acf531b7-8e56-4db9-990e-c96b966bd530": "Describe the CRAQ replication protocol, outlining the steps involved in handling a write request.  Specifically, address how version numbers are used to ensure data consistency and how concurrent write requests are handled.  What is the role of RDMA in this process, and what happens if an RDMA Read operation times out?",
        "4e6116b1-a40c-4a96-8d74-7b0ab82c1093": "Explain the process of handling a write request, from its arrival at the head target (A) to its successful completion, including how the system manages concurrent writes and versioning.  Illustrate your answer with the example provided involving targets A, B, and C and the failure of B.  Specifically address how the system handles the failure of B and the subsequent forwarding of the write request to C.",
        "8e442f43-adbe-4607-8673-6faf20a84190": "Compare and contrast the handling of read requests when only a committed version of a chunk exists versus when both committed and pending versions exist.  Explain the role of the client in each scenario and how the system's response differs from a CRAQ implementation.  Also, describe the different public states a storage target can have and how these states impact read and write operations.",
        "f0f604d1-8fc9-4af3-99b8-17ec12f28eef": "A storage service detects that one of its managed storage targets has a public state of \"lastsrv\".  Describe the immediate action the service takes and explain the reasoning behind this action, referencing relevant information from the provided text.",
        "91bc7404-3766-4935-ba12-f4a4556f1024": "Explain the process a storage target undergoes when its local state transitions from `online` to `offline`, detailing the changes in public state, the role of the cluster manager, and the implications for data recovery.  Include a discussion of how the chain version and chain ordering are affected.",
        "d169a062-d2fc-40c4-903a-b1e3163abdd5": "Describe the data recovery process initiated when a storage service detects that all its storage targets are marked offline in the latest chain tables.  Detail the steps involved, including the roles of full-chunk-replace writes and the interaction with the predecessor service.",
        "3da8060e-8c79-4e40-b0b7-0c7321ad2551": "Explain the chunk management strategy employed by the chunk engine, focusing on the metadata handling (including the use of RocksDB and in-memory caching), the copy-on-write (COW) semantics of the *update* operation, and the physical block allocation and management mechanisms (including the role of `fallocate()`).",
        "76d5e2a8-a58b-4428-bd25-a151d2d771bf": "Describe the process the allocator uses to handle write operations, differentiating between general updates and append operations.  Be sure to include details about data handling, metadata updates, and the role of RocksDB.",
        "4de1fa9d-6647-4d2a-aa86-4e8dd8d8cae6": "Explain the strategy employed by the allocator to manage physical block sizes and their allocation.  Include details on the resource pools, bitmap usage, and the handling of situations where no available blocks exist.",
        "683b7b5e-e7ad-4bf1-9b8c-317dbbdde550": "DeepSeek V3 achieved significantly more efficient training compared to other models.  Describe at least three specific technical innovations employed by DeepSeek that contributed to this efficiency, and explain how each innovation reduced training costs or improved performance.",
        "b9971186-410b-4583-a306-a5b4f2378f01": "The article mentions conflicting cost figures for training DeepSeek V3.  Compare and contrast the stated cost of ~$5.5 million with the later mentioned cost of ~$6 million.  What are the potential reasons for this discrepancy, and what information is missing that would help clarify the actual cost?",
        "9580e8d1-8b62-4650-96c6-851eb640b670": "**Question 1 (Quantitative Analysis):**",
        "8cdd5d6b-2170-40f5-8133-0959a4388cfa": "Based on the provided data, calculate the discrepancy between the theoretically ideal GPU hours required for DeepSeek-V3 training (assuming perfect efficiency) and the actual GPU hours reported in the DeepSeek-V3 paper.  Explain the primary factors contributing to this difference.",
        "c1481712-a0a0-47f3-9841-6805c3261aa0": "Compare and contrast the training methodologies of DeepSeek-R1-Zero and DeepSeek-R1.  Specifically, address the role of reinforcement learning, supervised fine-tuning, and cold-start data in each model's development.",
        "11b04d53-1cc0-4cdb-aa98-f9e9d9b051fc": "Based on Figure 1, which benchmark shows the largest performance difference between DeepSeek-R1 and OpenAI-o1-1217, and what might account for this discrepancy?",
        "7f263c74-1c43-4da5-8066-155229e2d573": "**Compare and contrast the approaches used in DeepSeek-R1-Zero and DeepSeek-R1.  Specifically, address the differences in their handling of the \"cold start\" problem and the types of reinforcement learning employed.** (This question tests understanding of the core methodologies and their variations.)",
        "c0cbe4d5-2f6b-48c7-b7aa-e6f68d9abe20": "**Section 4.2 mentions \"Unsuccessful Attempts.\"  Without knowing the specifics of these attempts, propose *two* potential reasons why a reinforcement learning or distillation approach might fail to improve the reasoning capabilities of a language model, based on the information provided in the other sections of the paper.** (This question assesses critical thinking and application of knowledge from different parts of the document.)",
        "c24c526c-1faf-4c0e-a0ba-d87f472071c3": "Compare and contrast the approaches used to improve reasoning capabilities in DeepSeek-R1-Zero and DeepSeek-R1.  Specifically, address the role of reinforcement learning, supervised fine-tuning, and data sources in each model's development.",
        "6a8ea374-42d6-42fe-917f-c24be2904022": "The paper highlights the success of distilling knowledge from DeepSeek-R1 to smaller models.  Explain why direct distillation from DeepSeek-R1 proved more effective than applying reinforcement learning directly to a smaller model like Qwen2.5-32B, and discuss the implications of this finding for future research in LLM reasoning capabilities.",
        "ba98261e-bf48-4a80-a404-8bfb309859bb": "Compare and contrast the approaches used to develop DeepSeek-R1-Zero and DeepSeek-R1.  Specifically, address the roles of reinforcement learning (RL) and supervised fine-tuning (SFT) in each model's development.",
        "2144f0b3-53e9-4043-bd0b-9232960ea0a6": "DeepSeek-R1 demonstrates strong performance across various benchmarks.  Select *two* different benchmark categories (e.g., reasoning, coding, knowledge) and describe DeepSeek-R1's performance within those categories, comparing it to at least one other model mentioned in the text.  Be sure to cite specific benchmark names and scores where possible.",
        "e02aa6ce-ccbf-46a8-a692-ed5e2ed906cb": "Compare and contrast the performance of DeepSeek-R1 and DeepSeek-R1-Zero.  Specifically, address the training methodologies used for each and how these differences impact their performance on tasks requiring long-context understanding and non-exam-oriented queries.",
        "24014f53-f936-4fce-b6d3-a859947839da": "Explain the Group Relative Policy Optimization (GRPO) algorithm used in training DeepSeek-R1-Zero.  Why was this algorithm chosen, and what advantage does it offer compared to traditional reinforcement learning methods?",
        "c89d7ac7-8b05-4cad-a0a1-c0926527aceb": "Explain the Group Relative Policy Optimization (GRPO) algorithm, highlighting its key advantage over traditional reinforcement learning methods and how it addresses the issue of training costs.  Be sure to define all relevant variables in equation (1).",
        "ae652eae-21eb-4136-aa68-94e1dd04f15d": "What is the primary motivation behind using reinforcement learning without supervised data in the context of developing reasoning capabilities in Large Language Models (LLMs), as described in the provided text?",
        "9a6f359f-b981-4bed-8d57-9959be93140e": "Here are two diverse questions based on the provided text, suitable for a quiz or examination:",
        "ffc76758-45a8-41f0-9a46-8d55275804f9": "**(Short Answer):**  Explain the two main components of the rule-based reward system used to train DeepSeek-R1-Zero, and briefly describe how each component functions.",
        "cb697ae6-df09-4a9b-9e2e-530ab55991f3": "**Compare and contrast the performance of DeepSeek-R1-Zero and the OpenAI models (OpenAI-o1-mini and OpenAI-o1-0912) across the various benchmarks listed in Table 2.  Specifically, discuss the impact of majority voting on DeepSeek-R1-Zero's performance and its implications.** (This question tests understanding of comparative analysis and the impact of a specific technique).",
        "db06343b-1325-424f-906c-82e362ed638f": "**Explain the significance of DeepSeek-R1-Zero's self-evolution process using Reinforcement Learning (RL), focusing on the advantages of starting RL directly from the base model without supervised fine-tuning.  What insights does this approach offer regarding the model's development and its ability to handle complex reasoning tasks?** (This question tests understanding of the methodology and its implications for model development).",
        "845f06f7-45d3-4438-b0de-42b1d951485d": "Describe the \"aha moment\" observed during the training of DeepSeek-R1-Zero.  What specific behavioral change characterized this moment, and what does it reveal about the model's learning process and the potential of reinforcement learning?",
        "6680415a-dd9b-45ce-b32f-6969c728f5bb": "Explain how DeepSeek-R1-Zero's average response length changed during the reinforcement learning process, and discuss the relationship between this change and the model's ability to solve increasingly complex reasoning tasks.  What sophisticated behaviors emerged as a consequence of this increased computation time?",
        "68d698ef-fba3-4c5d-b5a6-0f9beb0942ad": "Here are two diverse questions based on the provided text, suitable for a quiz or examination:",
        "2b3564b4-a6b3-4024-aecd-5fd548b7ff72": "**Explain the \"aha moment\" described in Table 3 concerning DeepSeek-R1-Zero.  What does this reveal about the model's learning process and what is a key drawback mentioned regarding this model's output?** (This question tests understanding of the model's capabilities and limitations.)",
        "3616c951-fadf-4880-b15a-8e6cb6885f28": "Describe the method used to improve the readability of DeepSeek-R1's output, contrasting it with the challenges encountered in DeepSeek-R1-Zero.  Explain the specific format used for the cold-start data and the rationale behind it.",
        "8cbf3829-9c8b-4f84-8b94-ef9e6405b3b6": "Explain the role of rejection sampling in the creation of the supervised fine-tuning (SFT) data for DeepSeek-V3.  What criteria were used to filter out undesirable model outputs during this process, and why were these criteria implemented?",
        "a521d655-c505-467c-9e11-82577516fbca": "Describe the two-stage training process used to improve the model's helpfulness, harmlessness, and reasoning capabilities.  Specify the techniques used in each stage and the types of data employed.",
        "04b069a3-e596-45eb-b96e-46a11862e0e6": "The authors chose not to include a reinforcement learning (RL) stage in the distillation process for smaller models. Explain the rationale behind this decision and discuss a potential drawback of this approach.",
        "7892c005-29d1-4f2d-9303-426a5d5fb10f": "**Explain the evaluation methodology used for the Codeforces benchmark, including the data source and the metrics employed to assess model performance.**  (This tests understanding of a specific benchmark's evaluation process.)",
        "0a6ad92b-2336-4c1f-8c60-119bc513fcdd": "**Compare and contrast the prompt engineering strategies used for MMLU and MMLU-Redux.  How did the authors adapt the prompt format for DeepSeek-R1 in each case, and why?** (This tests understanding of different prompt engineering techniques and their rationale.)",
        "19fe49d4-a74b-40ae-8111-1c5ec262876e": "Compare and contrast the performance of DeepSeek-R1 and DeepSeek-V3 on at least three different benchmark tasks, highlighting the key differences and potential reasons for these variations based on the provided text.  Explain the impact of safety RL on DeepSeek-R1's performance in at least one specific benchmark.",
        "ee558096-0337-43e8-a77b-1c9dbde9dd65": "Based on Table 4 and the accompanying text, identify two benchmark tasks where DeepSeek-R1 shows a significant improvement over other models (including DeepSeek-V3 and OpenAI models), and explain what aspects of DeepSeek-R1's architecture or training contribute to this superior performance in those specific tasks.",
        "2b6e5fce-a052-4fc4-8327-be7118456a1f": "DeepSeek-R1 shows improved performance on several benchmarks.  Describe the specific improvements observed on *both* a factual benchmark and a benchmark assessing instruction following, explaining the training techniques contributing to each improvement.",
        "122c8f9a-aa3a-4a9c-b5ce-10a864419318": "DeepSeek-R1's performance on the Chinese SimpleQA benchmark is notably lower than on the English version. Explain the reason for this discrepancy and discuss the trade-off involved.",
        "9adaa492-1a8e-40dc-b354-cf084907a235": "Compare and contrast the performance of DeepSeek-R1 and OpenAI-o1-1217 across different task types (math, coding algorithms, engineering-oriented coding) as described in the provided text.  Explain any observed differences and the authors' rationale for these differences.",
        "21458fde-8f0d-4c29-8697-a63dddb1f097": "Based on Table 5, describe the impact of model size on the performance of the distilled DeepSeek-R1 models across the various reasoning benchmarks.  What conclusions can be drawn about the effectiveness of distillation in improving performance, and what further research is suggested by the authors?",
        "a7c1d6d0-4149-4211-bd94-6cf51b9a8efe": "Compare and contrast the performance of DeepSeek-R1-Zero-Qwen-32B and DeepSeek-R1-Distill-Qwen-32B on the AIME 2024 MATH-500 GPQA Diamond LiveCodeBench, referencing specific benchmark scores from Table 6.  What conclusions can be drawn about the effectiveness of model distillation versus large-scale reinforcement learning based on these results?",
        "b6b44a65-8e85-4f90-b1cb-902c4ce67ada": "The text discusses challenges encountered using Process Reward Models (PRM) and Monte Carlo Tree Search (MCTS) for reasoning model development.  Describe three limitations of PRM, and explain one significant challenge faced when scaling up the MCTS approach for training.  What fundamental difference between the problem domains of chess (as addressed by AlphaGo/AlphaZero) and natural language token generation contributes to this challenge?",
        "4b8a20b4-fe52-405b-97f1-b5bfa153b9b3": "Explain the challenges encountered in applying AlphaGo's self-search value model training approach to the DeepSeek model's token generation process, and why iteratively boosting performance through self-search proved difficult in this context.  Discuss the specific limitations mentioned.",
        "07a31cd9-09d4-415c-ae0f-9a9796ab7191": "DeepSeek-R1 demonstrates strong performance but has limitations. Identify three key areas where DeepSeek-R1 falls short compared to DeepSeek-V3 or other models, and describe the proposed future research directions to address these shortcomings."
    },
    "corpus": {
        "8f3d3da7-1a26-43ee-be78-ecc8244abd2f": "# DualPipe\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\n\nPipeline Bubbles and Memory Usage Comparison\n\n| Method    | Bubble                  | Parameter | Activation |\n|:---------:|:-----------------------:|:---------:|:----------:|\n| 1F1B      | (PP-1)(\ud835\udc39+\ud835\udc35)            | 1\u00d7        | PP         |\n| ZB1P      | (PP-1)(\ud835\udc39+\ud835\udc35-2\ud835\udc4a)         | 1\u00d7        | PP         |\n| DualPipe  | (PP/2-1)(\ud835\udc39&\ud835\udc35+\ud835\udc35-3\ud835\udc4a)     | 2\u00d7        | PP+1       |\n\n\ud835\udc39 denotes the execution time of a forward chunk, \ud835\udc35 denotes the execution time of a full backward chunk, \ud835\udc4a denotes the execution time of a \"backward for weights\" chunk, and \ud835\udc39&\ud835\udc35 denotes the execution time of two mutually overlapped forward and backward chunks.\n\n### About\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\n\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\n\n# Profiling Data in DeepSeek Infra\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.\n\n## Training\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\n\n## Inference\n### Prefilling\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 \u2019s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches \u2014 meaning that the same prompt may be split between them.\n\n### Decoding\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\n\n# Expert Parallelism Load Balancer (EPLB)\n\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\n\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\n\n## The Algorithm\n\nThe load balancing algorithm comes with two policies used for different cases.",
        "c1bd2613-9fca-415e-8f81-2d2942d20ac3": "As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\n\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\n\n## The Algorithm\n\nThe load balancing algorithm comes with two policies used for different cases.\n\n## Hierarchical Load Balancing\n\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\n\n### Global Load Balancing\n\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\n\n# Fire-Flyer File system\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\n\n- Performance and Usability\n\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\n\n- Diverse Workloads\n\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\n\n## Performance\n1. Peak throughput\n\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2\u00d7200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\n\n2. GraySort\n\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\n\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2\u00d7400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1\u00d7200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\n\n3. KVCache\n\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s.",
        "0fd25e83-7190-4ae8-a5ca-39be36987302": "Both phases read/write data from/to 3FS.\n\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2\u00d7400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1\u00d7200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\n\n3. KVCache\n\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.",
        "7436663f-076f-4f56-a44c-313b0a23d820": "# Design Notes\n\n## Design and implementation\n\nThe 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE).\n\nMetadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and clients. Multiple cluster managers are deployed and one of them is elected as the primary. Another manager is promoted as primary when the primary fails. Cluster configuration is typically stored in a reliable distributed coordination service, such as ZooKeeper or etcd. In our production environment, we use the same key-value store as file metadata to reduce dependencies.\n\nFile metadata operations (e.g. open or create files/directories) are sent to metadata services, which implement the file system semantics. Metadata services are stateless, since file metadata are stored in a transactional key-value store (e.g. FoundationDB). Clients can connect to any metadata service.\n\nEach storage service manages a few local SSDs and provides a chunk store interface. The storage service implements Chain Replication with Apportioned Queries (CRAQ) to ensure strong consistency. CRAQ\u2019s write-all-read-any approach helps to unleash the throughput of SSDs and RDMA network. A 3FS file is split into equally sized chunks, which are replicated over multiple SSDs.\n\nTwo clients are developed for applications: FUSE client and native client. Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client.\n\n## File system interfaces\n\nObject store is becoming a popular option for data analytics and machine learning. However, file system semantics and a unified namespace where files are organized in directories provide greater flexibility for applications.\n\n-   *Atomic directory manipulation* An object store can approximate hierarchical directory structures by using slashes (/) in object keys. However, it doesn\u2019t natively support operations like atomically moving files/directories, or recursively deleting entire directories. Actually a common pattern in our internal applications involves creating a temporary directory, writing files to it, and then moving the directory to its final location. When handling a large number of small files, the recursive delete for directories is crucial. Without it, applications have to traverse each directory and remove files one by one.\n\n-   *Symbolic and hard links* Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files.\n\n-   *Familiar interface* The file interface is well known and used everywhere. There is no need to learn a new storage API. Many datasets are stored as CSV/Parquet files. Adapting file-based data loaders to use the 3FS FUSE client or native client is straightforward.\n\n### Limitations of FUSE\n\nFUSE (Filesystem in Userspace) simplifies file system client development by redirecting I/O operations to user-space processes through the FUSE kernel module. It creates the illusion that applications are accessing the remote file system as if it were a local file system. However, it has performance limitations:\n\n-   *Memory copy overhead* The user-space file system daemon cannot access application memory. Data transfer between kernel and user spaces consumes memory bandwidth and increases end-to-end latency.\n\n-   *Primitive multi-threading support* When an application initiates I/O requests, FUSE places these requests into a multi-threaded shared queue, protected by a spin lock. The user-space file system daemon then retrieves and processes requests from this queue. Due to lock contention, FUSE\u2019s I/O processing capability fails to scale with the number of threads. Our benchmark results indicate that FUSE only handles approximately 400K 4KiB reads per second. Further increasing concurrency does not improve performance as lock contention intensifies. `perf` profiling reveals that the kernel-space spin lock consumes a significant amount of CPU time.\n\nMost applications, e.g. data analytics, perform large block writes on 3FS or they can buffer data in memory and flush it to 3FS when write buffer is full. However, FUSE on Linux 5.x does not support concurrent writes to the same file[^1]. Applications overcome this limitation by writing to multiple files concurrently, maximizing the total throughput.\n\nRead operations exhibit more complex patterns. Some training jobs require random access to dataset samples, with read sizes varying from a few kilobytes to several megabytes per sample. And samples are typically not 4K-aligned in files. Data loaders are specifically designed to fetch batches of samples. But they perform poorly when handling small random reads on FUSE-mounted 3FS. Bandwidth of SSDs and RDMA network are not fully utilized.",
        "58e336d2-498c-45a5-b2c0-ec4ae288143a": "`perf` profiling reveals that the kernel-space spin lock consumes a significant amount of CPU time.\n\nMost applications, e.g. data analytics, perform large block writes on 3FS or they can buffer data in memory and flush it to 3FS when write buffer is full. However, FUSE on Linux 5.x does not support concurrent writes to the same file[^1]. Applications overcome this limitation by writing to multiple files concurrently, maximizing the total throughput.\n\nRead operations exhibit more complex patterns. Some training jobs require random access to dataset samples, with read sizes varying from a few kilobytes to several megabytes per sample. And samples are typically not 4K-aligned in files. Data loaders are specifically designed to fetch batches of samples. But they perform poorly when handling small random reads on FUSE-mounted 3FS. Bandwidth of SSDs and RDMA network are not fully utilized.\n\n### Asynchronous zero-copy API\n\nImplementing the file system client as a VFS kernel module avoids performance issues mentioned above. But kernel module development is significantly more challenging than user-space system programming. Bugs are difficult to diagnose and can lead to catastrophic failures in production environments. For example, machines may crash and leave no log message for debugging. When upgrading a kernel module, all processes using the file system must be stopped cleanly; otherwise, a machine restart is required.\n\nFor these reasons, we have chosen to implement a native client within the FUSE daemon. This client offers an interface that supports asynchronous zero-copy I/O operations. File meta operations are still handled by FUSE daemon (e.g. open/close/stat files). Applications call `open()` to obtain a file descriptor (fd) and register it via native API. They can then perform I/O operations on the file with native client. This approach ensures consistency in metadata operations with the POSIX API, making it easier to migrate existing code.\n\nThe asynchronous, zero-copy API is inspired by Linux `io_uring`. Below are the key data structures in the API:\n\n-   *Iov* A large memory region for zero-copy read/write operations, shared between the user process and the native client. InfiniBand memory registration is managed by the client. In native API, all read data will be read into Iov, and all write data should be written to Iov before calling the API.\n\n-   *Ior* A small shared ring buffer for communication between user process and native client. The usage of Ior is similar to Linux `io_uring`, where the user process enqueues read/write requests, and the native client dequeues these requests for completion. The requests are executed in batches, with their sizes controlled by the `io_depth` parameter. Multiple batches are processed in parallel, whether from different rings or the same ring. However, multiple rings are still recommended for multi-threaded applications, as sharing a ring requires synchronization, which can impact performance.\n\nWithin the native client, multiple threads are spawned to fetch I/O requests from the Iors. These requests are batched and dispatched to storage services, reducing RPC overhead caused by small read requests.\n\n## File metadata store\n\n### Location of file chunks\n\n3FS divides file data into equally sized chunks and stripes them across multiple replication chains (replication chains and chain tables are defined in Section [Data placement](#data-placement)). Users can specify the chain table, chunk size, and stripe size for files on a per-directory basis. Each chunk is independently stored on multiple storage services, with its chunk ID generated by concatenating the file\u2019s inode id and chunk index.\n\nWhen creating a new file, the metadata service employs a round-robin strategy to select consecutive replication chains from the designated chain table, based on the stripe size. Next, a random seed is generated to shuffle the selected chains. This allocation strategy ensures balanced data distribution across chains and SSDs.\n\nWhen an application opens a file, the client contacts the meta service to obtain the file\u2019s data layout information. Then the client can independently compute chunk IDs and chains for data operations, minimizing the involvement of the meta service in the critical path.\n\n### File metadata on transactional key-value store\n\n3FS uses FoundationDB as its distributed storage system for metadata. FoundationDB provides a key-value store interface and supports transactions with Serializable Snapshot Isolation (SSI). 3FS stores all metadata as key-value pairs in FoundationDB. Meta services follow a stateless architecture, greatly enhancing maintainability by allowing administrators to seamlessly upgrade or restart services without disruption. When clients experience request failures or timeouts, they can automatically fail over to other available services.\n\nThe file system metadata primarily consists of two core structures: inodes and directory entries. Inodes store attribute information for files, directories, and symbolic links, each identified by a globally unique 64-bit identifier that increments monotonically.",
        "fd043759-508a-4c6e-8309-5c265f7c90d1": "When an application opens a file, the client contacts the meta service to obtain the file\u2019s data layout information. Then the client can independently compute chunk IDs and chains for data operations, minimizing the involvement of the meta service in the critical path.\n\n### File metadata on transactional key-value store\n\n3FS uses FoundationDB as its distributed storage system for metadata. FoundationDB provides a key-value store interface and supports transactions with Serializable Snapshot Isolation (SSI). 3FS stores all metadata as key-value pairs in FoundationDB. Meta services follow a stateless architecture, greatly enhancing maintainability by allowing administrators to seamlessly upgrade or restart services without disruption. When clients experience request failures or timeouts, they can automatically fail over to other available services.\n\nThe file system metadata primarily consists of two core structures: inodes and directory entries. Inodes store attribute information for files, directories, and symbolic links, each identified by a globally unique 64-bit identifier that increments monotonically. Inode keys are constructed by concatenating the \"INOD\" prefix with the inode id, which is encoded in little-endian byte order to spread inodes over multiple FoundationDB nodes. The inode values vary by its type:\n\n-   All inode types contain basic attributes: ownership, permissions, access/modification/change times.\n\n-   Additional attributes for file inodes: file length, chunk size, selected range in chain table, shuffle seed.\n\n-   Additional attributes for directory inodes: the parent directory\u2019s inode id, default layout configurations for subdirectories/files (chain table, chunk size, stripe size). The parent\u2019s inode id is required to detect loops when moving directories. When moving `dir_a/dir_b` to `dir_c/`, we need to ensure that `dir_c` is not a descendant of `dir_b`, which can be achieved by checking all ancestors of `dir_c` upward.\n\n-   Additional attributes for symbolic link inodes: target path string.\n\nDirectory entry keys are composed of a \"DENT\" prefix, the parent inode ID, and the entry name. Directory entry values store the target inode id and inode type. All entries within a directory naturally form a contiguous key range, allowing efficient directory listing via range queries.\n\nThe meta operations leverage FoundationDB\u2019s transactions:\n\n-   Read-only transactions used for metadata queries: fstat, lookup, listdir etc.\n\n-   Read-write transactions used for metadata updates: create, link, unlink, rename etc.\n\nFor write transactions, FoundationDB tracks the read/write key sets to form conflict detection sets. When concurrent transaction conflicts are detected, the meta service automatically retries the transaction. This design enables multiple meta services to process requests in parallel while maintaining file system metadata consistency.\n\n### Dynamic file attributes\n\nOn most local file systems, deleting an opened file is deferred until all associated file descriptors are closed. Consequently, it is necessary to track all file descriptors of the file. Training jobs open a large number of files during startup. Storing all file descriptors would impose heavy load on meta service and FoundationDB. Since training jobs do not depend on this feature, 3FS does not track file descriptors opened in read-only mode.\n\n3FS maintains a file session for each file descriptor (fd) opened in write mode since deleting write opened files may lead to unreclaimable garbage chunks from concurrent writes. When a file with active write sessions is deleted, meta service delays the deletion until all its fds are closed. To prevent lingering sessions from offline clients, the 3FS meta service periodically checks client liveness and cleans up sessions of offline clients.\n\nThe file length is stored in the inode. For files being actively updated, the length stored in inode may diverge from the actual length. Clients periodically (5 seconds by default) report to meta service maximum write position of each file opened in write mode. If this position exceeds the length in inode and there is no concurrent truncate operation, this position is adopted as the new file length.\n\nDue to the possibility of concurrent writes from multiple clients, the method described above ensures only eventual consistency for file lengths. When processing close/fsync operations, the meta service obtains the precise file length by querying the ID and length of the last chunk from the storage service. Since file data is striped across multiple chains, this operation incurs non-negligible overhead.\n\nConcurrent updates to the same file\u2019s length by multiple meta services may cause transaction conflicts and lead to repeated file length computation. To mitigate this, meta service distributes file length update tasks across multiple meta services using inode IDs and the rendezvous hash algorithm.\n\nOur production environments use a large stripe size: 200. For small files, the number of chains containing file chunks is well below this number. The number of potentially used chains is stored in file inode and used as a hint when updating the length.",
        "3f49a332-58ad-4e52-88a9-4c8e9e74e2eb": "Due to the possibility of concurrent writes from multiple clients, the method described above ensures only eventual consistency for file lengths. When processing close/fsync operations, the meta service obtains the precise file length by querying the ID and length of the last chunk from the storage service. Since file data is striped across multiple chains, this operation incurs non-negligible overhead.\n\nConcurrent updates to the same file\u2019s length by multiple meta services may cause transaction conflicts and lead to repeated file length computation. To mitigate this, meta service distributes file length update tasks across multiple meta services using inode IDs and the rendezvous hash algorithm.\n\nOur production environments use a large stripe size: 200. For small files, the number of chains containing file chunks is well below this number. The number of potentially used chains is stored in file inode and used as a hint when updating the length. It starts with an initial value of 16 and is doubled each time additional file chunks are written to more chains. This allows us to avoid querying all 200 chains when updating lengths of small files. This optimization can also be extended to the deletion of small files.\n\n## Chunk storage system\n\nThe design goal of chunk storage system is to achieve the highest bandwidth possible even when there are storage medium failures. The read/write throughput of 3FS should scale linearly with the number of SSDs and bisection network bandwidth between clients and storage services. Applications access storage services in a locality-oblivious manner.\n\n### Data placement\n\nEach file chunk is replicated over a chain of storage targets using chain replication with apportioned queries (CRAQ). In CRAQ write requests are sent to the head target and propagated along a chain. Read requests can be sent to any of the storage target. Usually the read traffic is evenly distributed among all targets in a chain for better load balance. Multiple storage targets are created on each SSD and the targets join different chains.\n\nSuppose there are 6 nodes: A, B, C, D, E, F. Each node has 1 SSD. Create 5 storage targets on each SSD: 1, 2, ... 5. Then there are 30 targets in total: A1, A2, A3, ..., F5. If each chunk has 3 replicas, a chain table is constructed as follows.\n\n| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) |\n| :---: | :-----: | :-------------: | :------: | :-------------: |\n|   1   |    1    |      `A1`       |   `B1`   |      `C1`       |\n|   2   |    1    |      `D1`       |   `E1`   |      `F1`       |\n|   3   |    1    |      `A2`       |   `B2`   |      `C2`       |\n|   4   |    1    |      `D2`       |   `E2`   |      `F2`       |\n|   5   |    1    |      `A3`       |   `B3`   |      `C3`       |\n|   6   |    1    |      `D3`       |   `E3`   |      `F3`       |\n|   7   |    1    |      `A4`       |   `B4`   |      `C4`       |\n|   8   |    1    |      `D4`       |   `E4`   |      `F4`       |\n|   9   |    1    |      `A5`       |   `B5`   |      `C5`       |\n|  10   |    1    |      `D5`       |   `E5`   |      `F5`       |\n\nEach chain has a version number. The version number is incremented if the chain is changed (e.g. a storage target is offline). Only the primary cluster manager makes changes to chain tables.\n\nA few chain tables can be constructed to support different data placement requirements. For example, two chain tables can be created, one for batch/offline jobs and another for online services. The two tables consist of storage targets on mutually exclusive nodes and SSDs.\n\nLogically, the state of each chain changes independently. Each chain can be included in multiple chain tables. The concept of chain table is created to let metadata service pick a table for each file and stripe file chunks across chains in the table.",
        "6ae8a4e6-ceaf-418e-85dd-b6cf3961467c": "The version number is incremented if the chain is changed (e.g. a storage target is offline). Only the primary cluster manager makes changes to chain tables.\n\nA few chain tables can be constructed to support different data placement requirements. For example, two chain tables can be created, one for batch/offline jobs and another for online services. The two tables consist of storage targets on mutually exclusive nodes and SSDs.\n\nLogically, the state of each chain changes independently. Each chain can be included in multiple chain tables. The concept of chain table is created to let metadata service pick a table for each file and stripe file chunks across chains in the table.\n\n### Balanced traffic during recovery\n\nSuppose read traffic is evenly distributed among all storage targets in the above chain table. When A fails its read requests would be redirected to B and C. Under heavy load the read bandwidth of B, C is immediately saturated and B, C become the bottleneck of the entire system. Replacing a failed SSD and syncing data to the new SSD can take several hours. The read throughput is impaired during this period.\n\nTo reduce the performance impact, we can have more SSDs share the redirected traffic. In the following chain table, A is paired with every other SSDs. When A fails, each of the other SSDs receives 1/5 of A\u2019s read traffic.\n\n| Chain | Version | Target 1 (head) | Target 2 | Target 3 (tail) |\n| :---: | :-----: | :-------------: | :------: | :-------------: |\n|   1   |    1    |      `B1`       |   `E1`   |      `F1`       |\n|   2   |    1    |      `A1`       |   `B2`   |      `D1`       |\n|   3   |    1    |      `A2`       |   `D2`   |      `F2`       |\n|   4   |    1    |      `C1`       |   `D3`   |      `E2`       |\n|   5   |    1    |      `A3`       |   `C2`   |      `F3`       |\n|   6   |    1    |      `A4`       |   `B3`   |      `E3`       |\n|   7   |    1    |      `B4`       |   `C3`   |      `F4`       |\n|   8   |    1    |      `B5`       |   `C4`   |      `E4`       |\n|   9   |    1    |      `A5`       |   `C5`   |      `D4`       |\n|  10   |    1    |      `D5`       |   `E5`   |      `F5`       |\n\nTo achieve maximum read throughput during recovery, the load balance problem can be formulated as a balanced incomplete block design. The optimal solution is obtained by using integer programming solver.\n\n### Data replication\n\nCRAQ is a write-all-read-any replication protocol optimized for read-heavy workloads. Utilizing read bandwidth of all replicas is critical to achieve highest read throughput in an all-flash storage system.\n\nWhen a write request is received by a storage service, it goes through the following steps:\n\n1.  The service checks if the chain version in write request matches with the latest known version; reject the request if it\u2019s not. The write request could be sent by a client or a predecessor in the chain.\n\n2.  The service issues RDMA Read operations to pull write data. If the client/predecessor fails, the RDMA Read operations may time out and the write is aborted.\n\n3.  Once the write data is fetched into local memory buffer, a lock for the chunk to be updated is acquired from a lock manager. Concurrent writes to the same chunk are blocked. All writes are serialized at the head target.\n\n4.  The service reads the committed version of the chunk into memory, applies the update, and stores the updated chunk as a pending version. A storage target may store two versions of a chunk: a committed version and a pending version. Each version has a monotonically-increasing version number. The version numbers of committed version and pending versions are `v` and `u` respectively, and satisfy `u = v + 1`.\n\n5.  If the service is the tail, the committed version is atomically replaced by the pending version and an acknowledgment message is sent to the predecessor.",
        "41a6f965-6f8d-4aad-b9d0-0bc6a1784724": "If the client/predecessor fails, the RDMA Read operations may time out and the write is aborted.\n\n3.  Once the write data is fetched into local memory buffer, a lock for the chunk to be updated is acquired from a lock manager. Concurrent writes to the same chunk are blocked. All writes are serialized at the head target.\n\n4.  The service reads the committed version of the chunk into memory, applies the update, and stores the updated chunk as a pending version. A storage target may store two versions of a chunk: a committed version and a pending version. Each version has a monotonically-increasing version number. The version numbers of committed version and pending versions are `v` and `u` respectively, and satisfy `u = v + 1`.\n\n5.  If the service is the tail, the committed version is atomically replaced by the pending version and an acknowledgment message is sent to the predecessor. Otherwise, the write request is forwarded to the successor. When the committed version is updated, the current chain version is stored as a field in the chunk metadata.\n\n6.  When an acknowledgment message arrives at a storage service, the service replaces the committed version with the pending version and continues to propagate the message to its predecessor. The local chunk lock is then released.\n\nSuppose there are 3 targets in the chain: `A, B, C`. A write request has just entered step 5 at `A`. `A` forwards the request to successor `B`. Then `B` instantly fails and the forwarded write request is lost. When cluster manager detects `B`\u2019s failure, it marks `B` as offline and moves it to the end of chain and broadcasts the updated chain table. Once `A` receives the latest chain table, it forwards the write request to the new successor `C`. `C` may not receive the latest chain table yet and rejects the request. But `A` can keep forwarding the request to `C`. Eventually `C` gets the latest chain table and accepts the request.\n\nWhen a read request arrives at a storage service:\n\n1.  When the service only has a committed version of the chunk, this version is returned to the client.\n\n2.  Unlike CRAQ, our implementation does not issue version query to the tail target. When there are both committed and pending versions, the service replies a special status code to notify the client. The client may wait for a short interval and retry. Or the client can issue a relaxed read request to get the pending version.\n\n### Failure detection\n\nThe cluster manager relies on heartbeats to detect fail-stop failures. Cluster manager declares a service failed if it does not receive heartbeats from it for a configurable interval (e.g. T seconds). A service stops processing requests and exits if it cannot communicate with cluster manager for T/2 seconds. The heartbeat can be seen as a request to \\*renew a lease\\* granted by the manager.\n\nThe metadata services are stateless. The list of online meta services provided by cluster manager is a simple service discovery mechanism that helps clients create connections to metadata services. If one meta service is down, the clients may switch to any other metadata service.\n\nCluster manager plays a more critical role in membership changes of storage services. It maintains a global view of chain tables and storage targets\u2019 states. Each storage target has a public state and a local state.\n\nPublic state indicates if it\u2019s ready to serve read requests and if write requests would be propagated to it. Public states are stored with chain tables and distributed to services and clients.\n\n| Public State | Read | Write | Notes                                           |\n| :----------- | :--: | :---: | :---------------------------------------------- |\n| serving      |  Y   |   Y   | service alive and serving client requests       |\n| syncing      |  N   |   Y   | service alive and data recovery is in progress  |\n| waiting      |  N   |   N   | service alive and data recovery not started yet |\n| lastsrv      |  N   |   N   | service down and it was the last serving target |\n| offline      |  N   |   N   | service down or storage medium failure          |\n\nLocal state is only known by storage services and cluster manager, and it\u2019s stored in the memory of cluster manager. If a storage target has medium failure, the related service sets the target\u2019s local state to offline in heartbeat. If a storage service is down, storage targets managed by the service are marked offline.",
        "74689f65-85e6-4588-8688-7e5cee0c2366": "| Public State | Read | Write | Notes                                           |\n| :----------- | :--: | :---: | :---------------------------------------------- |\n| serving      |  Y   |   Y   | service alive and serving client requests       |\n| syncing      |  N   |   Y   | service alive and data recovery is in progress  |\n| waiting      |  N   |   N   | service alive and data recovery not started yet |\n| lastsrv      |  N   |   N   | service down and it was the last serving target |\n| offline      |  N   |   N   | service down or storage medium failure          |\n\nLocal state is only known by storage services and cluster manager, and it\u2019s stored in the memory of cluster manager. If a storage target has medium failure, the related service sets the target\u2019s local state to offline in heartbeat. If a storage service is down, storage targets managed by the service are marked offline.\n\n| Local State | Notes                                                |\n| :---------- | :--------------------------------------------------- |\n| up-to-date  | service alive and serving client requests            |\n| online      | service alive and target in syncing or waiting state |\n| offline     | service down or storage medium failure               |\n\nA storage target can change from one public state to another in response to the latest local state. The local state plays the role of a triggering event. The cluster manager periodically scans every chain and updates the public states of targets on the chain according to a state-transition table.\n\n-   The chain version is incremented if the chain is updated.\n\n-   If a storage target is marked offline, it\u2019s moved to the end of chain.\n\n-   If a storage service finds public state of any local storage target is lastsrv or offline, it exits immediately. The service may be isolated from the cluster manager by network partition error.\n\n-   Once the data recovery of a storage target in syncing state is completed, the storage service set the target\u2019s local state to up-to-date in subsequent heartbeat messages sent to cluster manager.\n\n| Local State | Current Public State | Predecessor\u2019s Public State | Next Public State |\n| :---------- | :------------------- | :------------------------- | :---------------- |\n| up-to-date  | serving              | (any)                      | serving           |\n|             | syncing              | (any)                      | serving           |\n|             | waiting              | (any)                      | waiting           |\n|             | lastsrv              | (any)                      | serving           |\n|             | offline              | (any)                      | waiting           |\n| online      | serving              | (any)                      | serving           |\n|             | syncing              | serving                    | syncing           |\n|             |                      | not serving                | waiting           |\n|             | waiting              | serving                    | syncing           |\n|             |                      | not serving                | waiting           |\n|             | lastsrv              | (any)                      | serving           |\n|             | offline              | (any)                      | waiting           |\n| offline     | serving              | has no predecessor         | lastsrv           |\n|             |                      | has predecessor            | offline           |\n|             | syncing              | (any)                      | offline           |\n|             | waiting              | (any)                      | offline           |\n|             | lastsrv              | (any)                      | lastsrv           |\n|             | offline              | (any)                      | offline           |\n\n### Data recovery\n\nWhen a storage service exits (e.g. process crashes or restarts during upgrade), or a storage medium failure occurs, all related storage targets will be marked as offline and moved to the end of chains by cluster manager. Once the service restarts, each target on the service enters into the recovery process independently. The entire recovery process overlaps with normal activity and minimizes any interruption.\n\nWhen a previously offline storage service starts:\n\n1.  The service periodically pulls latest chain tables from cluster manager. But it does not send heartbeats until all its storage targets have been marked offline in the latest chain tables. This ensures all its targets would go through the data recovery process.\n\n2.  When a write request arrives during recovery, the request is always a full-chunk-replace write. The local committed version is updated and any existing pending version is abandoned. Since current service is the tail, an acknowledgment message is sent to the predecessor. The full state of the predecessor is copied to the returning service through a continuous stream of full-chunk-replace writes.\n\n3.  Before the data recovery of a storage target starts, the predecessor sends a dump-chunkmeta request to the returning service. Then the service iterates the local chunk metadata store to collect the ids, chain versions and committed/pending version numbers of all chunks on the target, and replies the collected metadata to the predecessor.\n\n4.",
        "ccb3d584-b4e1-4d61-a4f5-001c517874e4": "The service periodically pulls latest chain tables from cluster manager. But it does not send heartbeats until all its storage targets have been marked offline in the latest chain tables. This ensures all its targets would go through the data recovery process.\n\n2.  When a write request arrives during recovery, the request is always a full-chunk-replace write. The local committed version is updated and any existing pending version is abandoned. Since current service is the tail, an acknowledgment message is sent to the predecessor. The full state of the predecessor is copied to the returning service through a continuous stream of full-chunk-replace writes.\n\n3.  Before the data recovery of a storage target starts, the predecessor sends a dump-chunkmeta request to the returning service. Then the service iterates the local chunk metadata store to collect the ids, chain versions and committed/pending version numbers of all chunks on the target, and replies the collected metadata to the predecessor.\n\n4.  When a sync-done message arrives, the service knows that the storage target is up-to-date. It sets local state of the target to up-to-date in heartbeat messages sent to cluster manager.\n\nWhen a storage service finds a previously offline successor is online:\n\n1. The service starts to forward normal write requests to the successor. Clients may only update a portion of the chunk, but the forwarded write requests should contain the whole chunk, i.e. a full-chunk-replace write.\n\n2. The service sends a dump-chunkmeta request to the successor. Once the metadata of all chunks on the successor target are received, it collects the chunk metadata on its local target. Then it compares the two copies of chunk metadata to decide which chunks should be transferred.\n\n3. The selected chunks are transferred to the successor by issuing full-chunk-replace write requests.\n\n   -   The chunk lock is first acquired for each chunk.\n\n   -   The chain version, committed version number and chunk content are read and transferred to successor by sending a full-chunk-replace request.\n\n   -   The chunk lock is released.\n\n   4\\. When all required chunks have been transferred, a sync-done message is sent to the successor.\n\nThe rules used to decide which chunks should be transferred are:\n\n-   If a chunk only exists on the local target, it should be transferred.\n\n-   If a chunk only exists on the remote target, it should be removed.\n\n-   If the chain version of local chunk replica is greater than that of the remote chunk replica, it should be transferred.\n\n-   If the chain versions of local/remote chunk replicas are the same but local committed version number does not equal to the remote pending version number, it should be transferred.\n\n-   Otherwise, two chunk replicas are either the same or being updated by in-progress write requests.\n\n### Chunks and the metadata\n\nFile chunks are stored in the chunk engine. On each SSD, the persistent storage of the chunk engine consists of a fixed number of data files for storing chunk data, and a RocksDB instance for maintaining chunk metadata and other system information. Additionally, the chunk engine maintains an in-memory cache of chunk metadata to enhance query performance. A chunk allocator is implemented for fast allocation of new chunks. The chunk engine interface provides thread-safe access through the following operations:\n\n1.  *open/close* Initializes the engine by loading metadata from RocksDB and reconstructing chunk allocator states.\n\n2.  *get* Retrieves chunk metadata and reference-counted handle through a hashmap cache, enabling concurrent access with O(1) average complexity.\n\n3.  *update* Implements copy-on-write (COW) semantics by allocating new chunks before modifying data. Old chunks remain readable until all handles are released.\n\n4.  *commit* Commit the updated chunk metadata to RocksDB via write batches to ensure atomic updates; synchronously refresh the chunk metadata cache.\n\nThe chunk data will ultimately be stored on physical blocks. Physical block sizes range from 64KiB to 64MiB in increments of powers of two, totaling 11 distinct sizes. The allocator will assign physical blocks whose sizes most closely match the actual chunk size. A resource pool is constructed for each physical block size, with each pool containing 256 physical files. The usage status of physical blocks is maintained in memory using bitmaps. When a physical block is reclaimed, its bitmap flag is set to 0. The actual storage space of the block remains preserved and will be prioritized for subsequent allocations. When no available physical blocks remain, `fallocate()` will be used to allocate a contiguous large space in physical files, creating 256 new physical blocks - this approach helps reduce disk fragmentation.\n\nWhen performing write operations on a chunk, the allocator first assigns a new physical block. The system then reads existing chunk data into a buffer, applies the update, and writes the updated buffer to the newly allocated block.",
        "426389e0-295f-4302-bf65-c903692b0ce2": "Physical block sizes range from 64KiB to 64MiB in increments of powers of two, totaling 11 distinct sizes. The allocator will assign physical blocks whose sizes most closely match the actual chunk size. A resource pool is constructed for each physical block size, with each pool containing 256 physical files. The usage status of physical blocks is maintained in memory using bitmaps. When a physical block is reclaimed, its bitmap flag is set to 0. The actual storage space of the block remains preserved and will be prioritized for subsequent allocations. When no available physical blocks remain, `fallocate()` will be used to allocate a contiguous large space in physical files, creating 256 new physical blocks - this approach helps reduce disk fragmentation.\n\nWhen performing write operations on a chunk, the allocator first assigns a new physical block. The system then reads existing chunk data into a buffer, applies the update, and writes the updated buffer to the newly allocated block. An optimized process is implemented for appends, where data is directly added in-place at the end of the existing block. A new copy of metadata is constructed from the new block's location and existing chunk metadata. Subsequently, both the new chunk metadata and statuses of new and old physical blocks are atomically updated in RocksDB.\n\n[^1]: https://elixir.bootlin.com/linux/v5.4.284/source/fs/fuse/file.c#L1573",
        "f15188aa-76ab-4673-8419-485eccd306d2": "<source name=\"https://medium.com/@visithkumarapperuma/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07\">\n\nauthor - Visith Kumarapperuma\n\n# Deepseek V3: A Game-Changer in A.I. Here\u2019s Why It Matters\n\nCurrently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to competitors. But above all, they achieved it with a fraction of the training and inference cost.\nDeepSeek\u2019s AI Assistant overtook ChatGPT to become the most downloaded free app on the U.S. App Store. This development has led to market concerns about A.I. investments to major U.S. tech companies. Impacting share prices of tech firms including Nvidia.\n\n## So what made Deepseek such a big impact to A.I. ?\nThe significance of Deepseek as a disruptor in the industry lies in its approach. Unlike other companies that pushed for better hardware, Deepseek improved the algorithms. Thus achieving better results at a software level.\nNote that the following details are for the Deepseek V3 model.\n\u2022 Deepseek said it trained a model using a data centre of some 2,000 of Nvidia H800 GPUs.\n\u2022 Time duration 2 months with the cost of the *final training run being ~$5.5 million\nThis ~$5.5M reflects the \u201crental\u201d cost for the GPU hours needed to train DeepSeek\u2011V3. It does not include:\n1. The capital expenditure for owning the hardware.\n2. Costs associated with prior research, ablation studies, or experiments on alternative architectures/algorithms/data.\n\n### Deepseek made training more efficient (45 times more efficient)\n- Use 8-bit instead of 32-bit to save memory.\n- Compress key value indices which eat up a lot of VRAM; they got 93% compression ratios.\n- Do multi-token prediction instead of single-token prediction -> doubled inference speeds\n- The MOE model decomposes a big model into small models that can run on consumer-grade hardware.\n\n## Summary of how Deepseek v3 was so efficient at training the frontier model\n1. Model Architecture\nThe model employs a Mixture-of-Experts (MoE) architecture, where only 37B parameters fire for each token out of the total 671B. This sparse activation significantly reduces compute requirements compared to dense models.\nThe model uses Multi-head Latent Attention (MLA). This compresses the Key-Value cache, reducing memory usage and enabling more efficient training.\n2. FP8 Mixed Precision Training:\nThey implemented an FP8 mixed precision training framework. Which reduces memory usage and accelerates training compared to higher precision formats.\nReduced memory footprint by up to 50% compared to traditional FP16/FP32 formats.\nThey use fine-grained quantisation strategies and increased accumulation precision to maintain accuracy.\n3. Load Balancing Strategy\nThey pioneered an auxiliary loss-free strategy for load balancing in the MoE architecture. This improved performance without the drawbacks of traditional auxiliary loss methods.\n4. Training Framework\nThey developed a custom training framework called HAI-LLM with several optimisations:\nDualPipe algorithm for efficient pipeline parallelism. This reduces pipeline bubbles and overlapping computation and communication.\nEfficient cross-node all-to-all communication kernels to fully utilise network bandwidth.\nCareful memory optimisations to avoid using costly tensor parallelism.\n\n## Breakdown of the costs of the Deepseek v3 model\nDeepseek\u2019s flagship model v3 showcases an architecture with a 671B parameter MOE (Mixture of Agents) with 37B active parameters per token\n- Their success stems from breakthrough engineering: using MoE architecture, implementing FP8 mixed precision training, and developing a custom HAI-LLM framework.\n- Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet.\n- For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead.\n- Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million\n- the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens.\n`So how true is the claim of $5.5 million, or is it another marketing trick?`\n\n1.",
        "9bdccab3-7157-479e-b784-fda1f8d22a1f": "- Deepseek excels at reasoning and math, surpassing GPT-4 and Claude 3.5 Sonnet.\n- For writing and coding tasks, Claude 3.5 Sonnet maintains a slight lead.\n- Deepseek pre-trained this model on 14.8 trillion high-quality data, taking 2,788,000 GPU hours on the Nvidia h800s cluster, costing around only $6 million\n- the Llama 403b was trained on 11x of that, taking 30,840,000 GPU hours, also on 15 trillion tokens.\n`So how true is the claim of $5.5 million, or is it another marketing trick?`\n\n1. Underlying FLOP calculations\nModel Details:\n- Active Parameters: 37B (using FP8 precision)\n- FLOPs per token: Using the rule of thumb \u201c6 FLOPs per parameter per token.\u201d\n`37B\u00d76 = 222B FLOPs per token`\n- Total Training Tokens: Approximately 14.8 trillion tokens\n- Total FLOPs required:\n`222 B FLOPs/token\u00d714.8 T tokens \u2248 3.3\u00d710\u00b2\u2074 FLOPs`\n### GPU FLOP Capacity (H800/H100):\nAn H100 is roughly estimated to deliver about.\n3.958\u00d710\u00b9\u2075 FLOPs (per second or per some standardised interval \u2014 here used as a comparative metric).\nIdeal (Perfect Efficiency) GPU hours.\n(Dividing total required FLOPs by per\u2011GPU capability gives)\n`3.3\u00d710\u00b2\u2074 / 3.958\u00d710\u00b9\u2075 \u200b\u2248 8.33\u00d710\u2078 seconds\u21d2\u22480.4 million GPU hour`\nNote: This \u201cperfect efficiency\u201d scenario is a lower bound. Real-world training is less efficient.\n2. Adjusting for Real\u2011World Inefficiencies (Comparison with Llama 3.1)\nReference Model: Llama 3.1 (405B parameters, 15 T tokens) reportedly required 30.84 M GPU hours in practice.\nRecalculating FLOPs for Llama 3.1:\n`Using the same math: 3.64\u00d710\u00b2\u2075 FLOPs required`\nScaling Efficiency\nUsing the ratio of FLOPs needed for DeepSeek\u2011V3 versus Llama 3.1. and assuming similar inefficiencies.\nThe estimate adjusts to roughly 2.79M GPU hours for DeepSeek\u2011V3 training.\n3. DeepSeek\u2011V3 Reported Training Breakdown\nAccording to the DeepSeek\u2011V3 paper\nPre\u2011training Stage:\n- Per Trillion Tokens: 180K H800 GPU hours\n- Overall Pre\u2011training: Total of 2,664K GPU hours\n- This stage was completed in less than two months using a cluster of 2,048 H800 GPUs.\nContext Length Extension:\n- Additional 119K GPU hours\nPost\u2011training:\n- An extra 5K GPU hours\nTotal GPU Hours:\n`2,664 K+119 K+5 K\u22482.788M GPU hours`\n4. Cost Estimation\nAssumed GPU Rental Price: $2 per GPU hour\nTotal Rental Cost:\n`2.788M GPU hours\u00d7$2/hour\u2248$5.576 million`\nas stated in Deepseek paper\nDuring the pre\u2011training stage, training DeepSeek\u2011V3 on each trillion tokens requires only 180K H800 GPU hours\u2026 Consequently, our pre\u2011training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post\u2011training, DeepSeek\u2011V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M.\n5. Summary\nTheoretical (Perfect Efficiency) Estimate: ~0.4 M GPU hours (using idealised FLOP counts and assuming perfect hardware utilisation0\nAdjusted (Real\u2011World) Estimate (via Llama 3.1 comparison): ~2.79 GPU hours\nDeepSeek\u2011V3 Reported Breakdown:\nPre\u2011training: 2,664K GPU hours\nContext Extension: 119K GPU hours\nPost\u2011training: 5K GPU hours\nTotal: ~2.788 M GPU hours\n### Cost (at $2 per GPU hour): ~$5.576 million",
        "71c22e84-2f31-4330-b398-ab9afb7b7c96": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\nAIME 2024\n(Pass@1)\nCodeforces\n(Percentile)\nGPQA Diamond\n(Pass@1)\nMATH-500\n(Pass@1)\nMMLU\n(Pass@1)\nSWE-bench Verified\n(Resolved)\n0\n20\n40\n60\n80\n100Accuracy / Percentile (%)\n79.8\n96.3\n71.5\n97.3\n90.8\n49.2\n79.2\n96.6\n75.7\n96.4\n91.8\n48.9\n72.6\n90.6\n62.1\n94.3\n87.4\n36.8\n63.6\n93.4\n60.0\n90.0\n85.2\n41.6\n39.2\n58.7 59.1\n90.2\n88.5\n42.0\nDeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1 |Benchmark performance of DeepSeek-R1.\narXiv:2501.12948v1  [cs.CL]  22 Jan 2025",
        "6d884f9d-84f6-48f0-a990-446e3e7adb47": "Contents\n1 Introduction 3\n1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2 Summary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Approach 5\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . . 5\n2.2.1 Reinforcement Learning Algorithm . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.2 Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2.3 Training Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6\n2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . . . . . . . . . . . . . . . 9\n2.3.1 Cold Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.3.2 Reasoning-oriented Reinforcement Learning . . . . . . . . . . . . . . . . . 10\n2.3.3 Rejection Sampling and Supervised Fine-Tuning . . . . . . . . . . . . . . . 10\n2.3.4 Reinforcement Learning for all Scenarios . . . . . . . . . . . . . . . . . . . 11\n2.4 Distillation: Empower Small Models with Reasoning Capability . . . . . . . . . . 11\n3 Experiment 11\n3.1 DeepSeek-R1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.2 Distilled Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4 Discussion 14\n4.1 Distillation v.s. Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Unsuccessful Attempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n5 Conclusion, Limitations, and Future Work 16\nA Contributions and Acknowledgments 20\n2",
        "832f04ff-aad1-46e5-8b20-8599a3d3eaed": "1. Introduction\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\ntowards Artificial General Intelligence (AGI).\nRecently, post-training has emerged as an important component of the full training pipeline.\nIt has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\nto user preferences, all while requiring relatively minimal computational resources against\npre-training. In the context of reasoning capabilities, OpenAI\u2019s o1 (OpenAI, 2024b) series models\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\nThought reasoning process. This approach has achieved significant improvements in various\nreasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\nof effective test-time scaling remains an open question for the research community. Several prior\nworks have explored various approaches, including process-based reward models (Lightman\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\nperformance comparable to OpenAI\u2019s o1 series models.\nIn this paper, we take the first step toward improving language model reasoning capabilities\nusing pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop\nreasoning capabilities without any supervised data, focusing on their self-evolution through\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\nDuring training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting\nreasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\non reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to\n71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\nof OpenAI-o1-0912.\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we introduce\nDeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\nnew record on the reasoning benchmarks among dense models.\n3",
        "6b8552bc-27c9-4fae-b36b-ab95356d3ad9": "1.1. Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\n\u2022 We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\na preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\nlong CoTs, marking a significant milestone for the research community. Notably, it is the\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized\npurely through RL, without the need for SFT. This breakthrough paves the way for future\nadvancements in this area.\n\u2022 We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\nstages aimed at discovering improved reasoning patterns and aligning with human pref-\nerences, as well as two SFT stages that serve as the seed for the model\u2019s reasoning and\nnon-reasoning capabilities. We believe the pipeline will benefit the industry by creating\nbetter models.\nDistillation: Smaller Models Can Be Powerful Too\n\u2022 We demonstrate that the reasoning patterns of larger models can be distilled into smaller\nmodels, resulting in better performance compared to the reasoning patterns discovered\nthrough RL on small models. The open source DeepSeek-R1, as well as its API, will benefit\nthe research community to distill better smaller models in the future.\n\u2022 Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models\nthat are widely used in the research community. The evaluation results demonstrate that\nthe distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-\nR1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-\ntionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n1.2. Summary of Evaluation Results\n\u2022 Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\nDeepSeek-V3, which could help developers in real world tasks.\n\u2022 Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\nsurpasses other closed-source models, demonstrating its competitive edge in educational\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\nwhere OpenAI-o1 surpasses 4o on this benchmark.\n4",
        "1697c52c-2fe0-4d59-afeb-3cda27527d30": "\u2022 Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\ngeneral question answering, editing, summarization, and more. It achieves an impressive\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\nbenchmarks.\n2. Approach\n2.1. Overview\nPrevious work has heavily relied on large amounts of supervised data to enhance model\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\nimproved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\nsmall dense models.\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data,\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\nhope this provides the community with valuable insights.\n2.2.1. Reinforcement Learning Algorithm\nGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\ntypically the same size as the policy model, and estimates the baseline from group scores instead.",
        "d1206964-c1e5-4a32-81bc-b6299e2afead": "However, these works\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data,\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\nhope this provides the community with valuable insights.\n2.2.1. Reinforcement Learning Algorithm\nGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\nSpecifically, for each question \ud835\udc5e, GRPO samples a group of outputs {\ud835\udc5c1, \ud835\udc5c2, \u00b7\u00b7\u00b7 , \ud835\udc5c\ud835\udc3a}from the old\npolicy \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 and then optimizes the policy model \ud835\udf0b\ud835\udf03 by maximizing the following objective:\nJ\ud835\udc3a\ud835\udc45\ud835\udc43\ud835\udc42 (\ud835\udf03)= E[\ud835\udc5e\u223c\ud835\udc43(\ud835\udc44), {\ud835\udc5c\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 \u223c\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)]\n1\n\ud835\udc3a\n\ud835\udc3a\u2211\ufe01\n\ud835\udc56=1\n\u0012\nmin\n\u0012 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56|\ud835\udc5e)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc56|\ud835\udc5e)\ud835\udc34\ud835\udc56, clip\n\u0012 \ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56|\ud835\udc5e)\n\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc5c\ud835\udc56|\ud835\udc5e), 1\u2212\ud835\udf00, 1+\ud835\udf00\n\u0013\n\ud835\udc34\ud835\udc56\n\u0013\n\u2212\ud835\udefdD\ud835\udc3e\ud835\udc3f\n\u0000\n\ud835\udf0b\ud835\udf03||\ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53\n\u0001\u0013\n, (1)\nD\ud835\udc3e\ud835\udc3f\n\u0000\n\ud835\udf0b\ud835\udf03||\ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53\n\u0001 =\n\ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53 (\ud835\udc5c\ud835\udc56|\ud835\udc5e)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56|\ud835\udc5e) \u2212log\n\ud835\udf0b\ud835\udc5f\ud835\udc52\ud835\udc53 (\ud835\udc5c\ud835\udc56|\ud835\udc5e)\n\ud835\udf0b\ud835\udf03(\ud835\udc5c\ud835\udc56|\ud835\udc5e) \u22121, (2)\nwhere \ud835\udf00 and \ud835\udefd are hyper-parameters, and \ud835\udc34\ud835\udc56 is the advantage, computed using a group of\nrewards {\ud835\udc5f1, \ud835\udc5f2, ... , \ud835\udc5f\ud835\udc3a}corresponding to the outputs within each group:\n\ud835\udc34\ud835\udc56 = \ud835\udc5f\ud835\udc56 \u2212m\ud835\udc52\ud835\udc4e\ud835\udc5b({\ud835\udc5f1, \ud835\udc5f2, \u00b7\u00b7\u00b7 , \ud835\udc5f\ud835\udc3a})\ns\ud835\udc61\ud835\udc51({\ud835\udc5f1, \ud835\udc5f2, \u00b7\u00b7\u00b7 , \ud835\udc5f\ud835\udc3a}) . (3)\n5",
        "3393c9c0-eb24-4dfa-981e-49d220dd70db": "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n<answer> answer here </answer>. User: prompt. Assistant:\nTable 1 |Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\nquestion during training.\n2.2.2. Reward Modeling\nThe reward is the source of the training signal, which decides the optimization direction of RL.\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\ntypes of rewards:\n\u2022 Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\nFor example, in the case of math problems with deterministic results, the model is required\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\nused to generate feedback based on predefined test cases.\n\u2022 Format rewards: In addition to the accuracy reward model, we employ a format reward\nmodel that enforces the model to put its thinking process between \u2018<think>\u2019 and \u2018</think>\u2019\ntags.\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\nreinforcement learning process, and retraining the reward model needs additional training\nresources and it complicates the whole training pipeline.\n2.2.3. Training Template\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.\nWe intentionally limit our constraints to this structural format, avoiding any content-specific\nbiases\u2014such as mandating reflective reasoning or promoting particular problem-solving strate-\ngies\u2014to ensure that we can accurately observe the model\u2019s natural progression during the RL\nprocess.\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\nPerformance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\nalgorithm in optimizing the model\u2019s performance over time.\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI\u2019s o1-0912\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\n6",
        "994f6543-43ce-4f4e-a5f1-b497f3fc5abe": "Model AIME 2024 MATH-500 GPQA LiveCode CodeForcesDiamond Bench\npass@1 cons@64 pass@1 pass@1 pass@1 rating\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\nOpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843\nDeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444\nTable 2 |Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\nbenchmarks.\nFigure 2 |AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\nfine-tuning data. This is a noteworthy achievement, as it underscores the model\u2019s ability to\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-\nR1-Zero can be further augmented through the application of majority voting. For example,\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero\u2019s performance\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\nmajority voting, highlights its strong foundational capabilities and its potential for further\nadvancements in reasoning tasks.\nSelf-evolution Process of DeepSeek-R1-ZeroThe self-evolution process of DeepSeek-R1-Zero\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\nautonomously. By initiating RL directly from the base model, we can closely monitor the model\u2019s\nprogression without the influence of the supervised fine-tuning stage. This approach provides\na clear view of how the model evolves over time, particularly in terms of its ability to handle\ncomplex reasoning tasks.\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\n7",
        "451aaefc-1c8c-49ef-8479-cbd2212a8c26": "Figure 3 |The average response length of DeepSeek-R1-Zero on the training set during the RL\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\nment throughout the training process. This improvement is not the result of external adjustments\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\nallowing the model to explore and refine its thought processes in greater depth.\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\nbehaviors as the test-time computation increases. Behaviors such as reflection\u2014where the model\nrevisits and reevaluates its previous steps\u2014and the exploration of alternative approaches to\nproblem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\nemerge as a result of the model\u2019s interaction with the reinforcement learning environment. This\nspontaneous development significantly enhances DeepSeek-R1-Zero\u2019s reasoning capabilities,\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\nAha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during\nthe training of DeepSeek-R1-Zero is the occurrence of an \u201caha moment\u201d. This moment, as\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\napproach. This behavior is not only a testament to the model\u2019s growing reasoning abilities\nbut also a captivating example of how reinforcement learning can lead to unexpected and\nsophisticated outcomes.\nThis moment is not only an \u201caha moment\u201d for the model but also for the researchers\nobserving its behavior. It underscores the power and beauty of reinforcement learning: rather\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\nright incentives, and it autonomously develops advanced problem-solving strategies. The\n\u201caha moment\u201d serves as a powerful reminder of the potential of RL to unlock new levels of\nintelligence in artificial systems, paving the way for more autonomous and adaptive models in\nthe future.\n8",
        "540a4e06-0733-4982-a9e5-5d83c12c733b": "Question: If \ud835\udc4e >1, then the sum of the real solutions of\n\u221a\ufe01\n\ud835\udc4e\u2212\u221a\n\ud835\udc4e+\ud835\udc65 = \ud835\udc65 is equal to\nResponse: <think>\nTo solve the equation\n\u221a\ufe01\n\ud835\udc4e\u2212\u221a\n\ud835\udc4e+\ud835\udc65 = \ud835\udc65, let\u2019s start by squaring both \u00b7\u00b7\u00b7\u0010\u221a\ufe01\n\ud835\udc4e\u2212\u221a\n\ud835\udc4e+\ud835\udc65\n\u00112\n= \ud835\udc652 =\u21d2 \ud835\udc4e\u2212\u221a\n\ud835\udc4e+\ud835\udc65 = \ud835\udc652.\nRearrange to isolate the inner square root term:\n(\ud835\udc4e\u2212\ud835\udc652)2 = \ud835\udc4e+\ud835\udc65 =\u21d2 \ud835\udc4e2 \u22122\ud835\udc4e\ud835\udc652 +(\ud835\udc652)2 = \ud835\udc4e+\ud835\udc65 =\u21d2 \ud835\udc654 \u22122\ud835\udc4e\ud835\udc652 \u2212\ud835\udc65+(\ud835\udc4e2 \u2212\ud835\udc4e)= 0\n. . .\nWait, wait. Wait. That\u2019s an aha moment I can flag here.\nLet\u2019s reevaluate this step-by-step to identify if the correct sum can be \u00b7\u00b7\u00b7\nWe started with the equation:\u221a\ufe01\n\ud835\udc4e\u2212\u221a\n\ud835\udc4e+\ud835\udc65 = \ud835\udc65\nFirst, let\u2019s square both sides:\n\ud835\udc4e\u2212\u221a\n\ud835\udc4e+\ud835\udc65 = \ud835\udc652 =\u21d2 \u221a\n\ud835\udc4e+\ud835\udc65 = \ud835\udc4e\u2212\ud835\udc652\nNext, I could square both sides again, treating the equation: \u00b7\u00b7\u00b7\n. . .\nTable 3 |An interesting \u201caha moment\u201d of an intermediate version of DeepSeek-R1-Zero. The\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,\nallowing us to witness the power and beauty of reinforcement learning.\nDrawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning\ncapabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\nseveral issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\nand language mixing. To make reasoning processes more readable and share them with the\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\ncold-start data.\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\nreasoning performance be further improved or convergence accelerated by incorporating a small\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\npipeline consists of four stages, outlined as follows.\n2.3.1. Cold Start\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\nZero outputs in a readable format, and refining the results through post-processing by human\nannotators.\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n9",
        "97c2dc10-d7a1-4455-8fb6-50a37a6e5277": "include:\n\u2022 Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\nfor reading. Responses may mix multiple languages or lack markdown formatting to\nhighlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,\nwe design a readable pattern that includes a summary at the end of each response and\nfilters out responses that are not reader-friendly. Here, we define the output format as\n|special_token|<reasoning_process>|special_token|<summary>, where the reasoning\nprocess is the CoT for the query, and the summary is used to summarize the reasoning\nresults.\n\u2022 Potential: By carefully designing the pattern for cold-start data with human priors, we\nobserve better performance against DeepSeek-R1-Zero. We believe the iterative training is\na better way for reasoning models.\n2.3.2. Reasoning-oriented Reinforcement Learning\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\nreinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses\non enhancing the model\u2019s reasoning capabilities, particularly in reasoning-intensive tasks such\nas coding, mathematics, science, and logic reasoning, which involve well-defined problems with\nclear solutions. During the training process, we observe that CoT often exhibits language mixing,\nparticularly when RL prompts involve multiple languages. To mitigate the issue of language\nmixing, we introduce a language consistency reward during RL training, which is calculated\nas the proportion of target language words in the CoT. Although ablation experiments show\nthat such alignment results in a slight degradation in the model\u2019s performance, this reward\naligns with human preferences, making it more readable. Finally, we combine the accuracy of\nreasoning tasks and the reward for language consistency by directly summing them to form the\nfinal reward. We then apply RL training on the fine-tuned model until it achieves convergence\non reasoning tasks.\n2.3.3. Rejection Sampling and Supervised Fine-Tuning\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\n(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which\nprimarily focuses on reasoning, this stage incorporates data from other domains to enhance the\nmodel\u2019s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we\ngenerate the data and fine-tune the model as described below.\nReasoning data We curate reasoning prompts and generate reasoning trajectories by perform-\ning rejection sampling from the checkpoint from the above RL training. In the previous stage,\nwe only included data that could be evaluated using rule-based rewards. However, in this stage,\nwe expand the dataset by incorporating additional data, some of which use a generative reward\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\neach prompt, we sample multiple responses and retain only the correct ones. In total, we collect\nabout 600k reasoning related training samples.\n10",
        "6f426411-03d2-4f6f-9622-6da27b3f2b44": "Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition,\nand translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of\nDeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential\nchain-of-thought before answering the question by prompting. However, for simpler queries,\nsuch as \u201chello\u201d we do not provide a CoT in response. In the end, we collected a total of\napproximately 200k training samples that are unrelated to reasoning.\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about\n800k samples.\n2.3.4. Reinforcement Learning for all Scenarios\nTo further align the model with human preferences, we implement a secondary reinforcement\nlearning stage aimed at improving the model\u2019s helpfulness and harmlessness while simultane-\nously refining its reasoning capabilities. Specifically, we train the model using a combination\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\nlearning process in math, code, and logical reasoning domains. For general data, we resort to\nreward models to capture human preferences in complex and nuanced scenarios. We build\nupon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\nassessment emphasizes the utility and relevance of the response to the user while minimizing\ninterference with the underlying reasoning process. For harmlessness, we evaluate the entire\nresponse of the model, including both the reasoning process and the summary, to identify and\nmitigate any potential risks, biases, or harmful content that may arise during the generation\nprocess. Ultimately, the integration of reward signals and diverse data distributions enables us\nto train a model that excels in reasoning while prioritizing helpfulness and harmlessness.\n2.4. Distillation: Empower Small Models with Reasoning Capability\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\nthe 800k samples curated with DeepSeek-R1, as detailed in \u00a72.3.3. Our findings indicate that\nthis straightforward distillation method significantly enhances the reasoning abilities of smaller\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\n14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its\nreasoning capability is slightly better than that of Llama-3.1.\nFor distilled models, we apply only SFT and do not include an RL stage, even though\nincorporating RL could substantially boost model performance. Our primary goal here is to\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\nstage to the broader research community.\n3. Experiment\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema\net al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,\n2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,\n2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\n11",
        "e604643f-6f16-45a2-87a8-f6a94e2de736": "2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 \u2013 2025-01), Codeforces 2, Chinese\nNational High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-\nematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we\nalso evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we\nadhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li\net al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we\nonly feed the final summary to evaluation to avoid the length bias. For distilled models, we\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\nLiveCodeBench.\nEvaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as\nMMLU, DROP , GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-\nevals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a\nzero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts\nare few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\nprotocols with default prompts provided by their creators. For code and math benchmarks, the\nHumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\nC#, JavaScript, TypeScript, PHP , and Bash). Model performance on LiveCodeBench is evaluated\nusing CoT format, with data collected between August 2024 and January 2025. The Codeforces\ndataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\nbenchmarks are measured using a \"diff\" format. DeepSeek-R1 outputs are capped at a maximum\nof 32,768 tokens for each benchmark.\nBaselines We conduct comprehensive evaluations against several strong baselines, including\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.\nSince accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\nmance based on official reports. For distilled models, we also compare the open-source model\nQwQ-32B-Preview (Qwen, 2024a).\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\nWe found that using greedy decoding to evaluate long-output reasoning models results in\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\ndefault to pass@\ud835\udc58evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\nSpecifically, we use a sampling temperature of 0.6 and a top- \ud835\udc5d value of 0.95 to generate \ud835\udc58\nresponses (typically between 4 and 64, depending on the test set size) for each question. Pass@1\nis then calculated as\npass@1 = 1\n\ud835\udc58\n\ud835\udc58\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc5d\ud835\udc56,\nwhere \ud835\udc5d\ud835\udc56 denotes the correctness of the \ud835\udc56-th response. This method provides more reliable\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\net al., 2022) using 64 samples, denoted as cons@64.\n1https://aider.chat\n2https://codeforces.com\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\n12",
        "5c54d19d-0296-45c9-b55b-a6a316950429": "3.1. DeepSeek-R1 Evaluation\nBenchmark (Metric)\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\nArchitecture - - MoE - - MoE\n# Activated Params - - 37B - - 37B\n# Total Params - - 671B - - 671B\nEnglish\nMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\nCode\nLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\nCodeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMath\nAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\nCNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8\nChinese\nCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8\nC-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8\nC-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7\nTable 4 |Comparison between DeepSeek-R1 and other representative models.\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\naccuracy of over 70%.",
        "f85accf4-70f0-47bd-ab5f-76f767480415": "This im-\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\naccuracy of over 70%.\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\nmodel\u2019s ability to follow format instructions. These improvements can be linked to the inclusion\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\nindicating DeepSeek-R1\u2019s strengths in writing tasks and open-domain question answering. Its\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\n13",
        "191167c7-1e6f-4594-8914-2e85b56bee55": "DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\nits robustness across multiple tasks.\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\ntraining data currently remains very limited.\n3.2. Distilled Model Evaluation\nModel AIME 2024 MATH-500 GPQA LiveCode CodeForcesDiamond Bench\npass@1 cons@64 pass@1 pass@1 pass@1 rating\nGPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759\nClaude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316\nDeepSeek-R1-Distill-Qwen-1.5B28.9 52.7 83.9 33.8 16.9 954\nDeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189\nDeepSeek-R1-Distill-Qwen-14B69.7 80.0 93.9 59.1 53.1 1481\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691\nDeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205\nDeepSeek-R1-Distill-Llama-70B70.0 86.7 94.5 65.2 57.5 1633\nTable 5 |Comparison of DeepSeek-R1 distilled models and other comparable models on\nreasoning-related benchmarks.\nAs shown in Table 5, simply distilling DeepSeek-R1\u2019s outputs enables the efficient DeepSeek-\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-\ntion. Additionally, we found that applying RL to these distilled models yields significant further\ngains. We believe this warrants further exploration and therefore present only the results of the\nsimple SFT-distilled models here.\n4. Discussion\n4.1. Distillation v.s. Reinforcement Learning\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\nresults. However, there is still one question left: can the model achieve comparable performance\nthrough the large-scale RL training discussed in the paper without distillation?\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,\ncode, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The\nexperimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n14",
        "2a24554d-cbc5-46ca-a5f8-f2f3090ab83d": "Model\nAIME 2024 MATH-500 GPQA Diamond LiveCodeBench\npass@1 cons@64 pass@1 pass@1 pass@1\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\nTable 6 |Comparison of distilled and RL Models on Reasoning-Related Benchmarks.\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\nthis paper require enormous computational power and may not even achieve the performance\nof distillation. Second, while distillation strategies are both economical and effective, advancing\nbeyond the boundaries of intelligence may still require more powerful base models and larger-\nscale reinforcement learning.\n4.2. Unsuccessful Attempts\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\nthe way. We share our failure experiences here to provide insights, but this does not imply that\nthese approaches are incapable of developing effective reasoning models.\nProcess Reward Model (PRM)PRM is a reasonable method to guide the model toward better\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\ndetermining whether the current intermediate step is correct is a challenging task. Automated\nannotation using models may not yield satisfactory results, while manual annotation is not con-\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\net al., 2024), its advantages are limited compared to the additional computational overhead it\nintroduces during the large-scale reinforcement learning process in our experiments.\nMonte Carlo Tree Search (MCTS)Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value\nmodel. Subsequently, we use the resulting question-answer pairs to train both the actor model\nand the value model, iteratively refining the process.\nHowever, this approach encounters several challenges when scaling up the training. First,\nunlike chess, where the search space is relatively well-defined, token generation presents an\n15",
        "67e5d050-2aa2-43aa-935e-9feee7862100": "exponentially larger search space. To address this, we set a maximum extension limit for each\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\ndirectly influences the quality of generation since it guides each step of the search process.\nTraining a fine-grained value model is inherently difficult, which makes it challenging for the\nmodel to iteratively improve. While AlphaGo\u2019s core success relied on training a value model to\nprogressively enhance its performance, this principle proves difficult to replicate in our setup\ndue to the complexities of token generation.\nIn conclusion, while MCTS can improve performance during inference when paired with a\npre-trained value model, iteratively boosting model performance through self-search remains a\nsignificant challenge.\n5. Conclusion, Limitations, and Future Work\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves\nperformance comparable to OpenAI-o1-1217 on a range of tasks.\nWe further explore distillation the reasoning capability to small dense models. We use\nDeepSeek-R1 as the teacher model to generate 800K training samples, and fine-tune several small\ndense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other\ndense models also achieve impressive results, significantly outperforming other instruction-\ntuned models based on the same underlying checkpoints.\nIn the future, we plan to invest in research across the following directions for DeepSeek-R1.\n\u2022 General Capability:Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\nMoving forward, we plan to explore how long CoT can be leveraged to enhance tasks in\nthese fields.\n\u2022 Language Mixing:DeepSeek-R1 is currently optimized for Chinese and English, which\nmay result in language mixing issues when handling queries in other languages. For\ninstance, DeepSeek-R1 might use English for reasoning and responses, even if the query is\nin a language other than English or Chinese. We aim to address this limitation in future\nupdates.\n\u2022 Prompting Engineering:When evaluating DeepSeek-R1, we observe that it is sensitive\nto prompts. Few-shot prompting consistently degrades its performance. Therefore, we\nrecommend users directly describe the problem and specify the output format using a\nzero-shot setting for optimal results.\n\u2022 Software Engineering Tasks:Due to the long evaluation times, which impact the effi-\nciency of the RL process, large-scale RL has not been applied extensively in software\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\nthis by implementing rejection sampling on software engineering data or incorporating\nasynchronous evaluations during the RL process to improve efficiency.\n16"
    },
    "relevant_docs": {
        "b2c84ac1-85e6-4ad0-8004-c6243b98a598": [
            "8f3d3da7-1a26-43ee-be78-ecc8244abd2f"
        ],
        "8b73ec09-a366-4296-837d-a13f14afb3cb": [
            "8f3d3da7-1a26-43ee-be78-ecc8244abd2f"
        ],
        "e958aa00-60ed-4f08-b75d-d856301587bf": [
            "c1bd2613-9fca-415e-8f81-2d2942d20ac3"
        ],
        "4031759d-e218-4d6c-9218-9e3abc7c04e1": [
            "c1bd2613-9fca-415e-8f81-2d2942d20ac3"
        ],
        "ed04d362-a62b-4b76-ad2d-828c48b5eecc": [
            "0fd25e83-7190-4ae8-a5ca-39be36987302"
        ],
        "de23b34d-1282-4374-988c-5fcfcaccce7c": [
            "0fd25e83-7190-4ae8-a5ca-39be36987302"
        ],
        "154dfc8b-e2af-42b2-a110-81a7a2e5b351": [
            "7436663f-076f-4f56-a44c-313b0a23d820"
        ],
        "2bdfe314-6f3c-4e04-9aec-3a20bb59caf7": [
            "7436663f-076f-4f56-a44c-313b0a23d820"
        ],
        "d661f6ad-890e-43c9-b823-827d20d3ff64": [
            "58e336d2-498c-45a5-b2c0-ec4ae288143a"
        ],
        "71f20926-37d3-4e90-96b2-abdda2fb23bb": [
            "58e336d2-498c-45a5-b2c0-ec4ae288143a"
        ],
        "4ccf1d63-24ea-4626-89cd-dbba5098a6ef": [
            "fd043759-508a-4c6e-8309-5c265f7c90d1"
        ],
        "11821d12-fcf7-45ac-9525-e9588835e0a4": [
            "fd043759-508a-4c6e-8309-5c265f7c90d1"
        ],
        "1c3fa216-3700-4b04-9e6b-8fb741e6028a": [
            "3f49a332-58ad-4e52-88a9-4c8e9e74e2eb"
        ],
        "0bb70dd8-3956-45a7-a319-56ef6bde0056": [
            "3f49a332-58ad-4e52-88a9-4c8e9e74e2eb"
        ],
        "4e9b97f1-07cd-4205-a88c-3e38aae461d0": [
            "6ae8a4e6-ceaf-418e-85dd-b6cf3961467c"
        ],
        "acf531b7-8e56-4db9-990e-c96b966bd530": [
            "6ae8a4e6-ceaf-418e-85dd-b6cf3961467c"
        ],
        "4e6116b1-a40c-4a96-8d74-7b0ab82c1093": [
            "41a6f965-6f8d-4aad-b9d0-0bc6a1784724"
        ],
        "8e442f43-adbe-4607-8673-6faf20a84190": [
            "41a6f965-6f8d-4aad-b9d0-0bc6a1784724"
        ],
        "f0f604d1-8fc9-4af3-99b8-17ec12f28eef": [
            "74689f65-85e6-4588-8688-7e5cee0c2366"
        ],
        "91bc7404-3766-4935-ba12-f4a4556f1024": [
            "74689f65-85e6-4588-8688-7e5cee0c2366"
        ],
        "d169a062-d2fc-40c4-903a-b1e3163abdd5": [
            "ccb3d584-b4e1-4d61-a4f5-001c517874e4"
        ],
        "3da8060e-8c79-4e40-b0b7-0c7321ad2551": [
            "ccb3d584-b4e1-4d61-a4f5-001c517874e4"
        ],
        "76d5e2a8-a58b-4428-bd25-a151d2d771bf": [
            "426389e0-295f-4302-bf65-c903692b0ce2"
        ],
        "4de1fa9d-6647-4d2a-aa86-4e8dd8d8cae6": [
            "426389e0-295f-4302-bf65-c903692b0ce2"
        ],
        "683b7b5e-e7ad-4bf1-9b8c-317dbbdde550": [
            "f15188aa-76ab-4673-8419-485eccd306d2"
        ],
        "b9971186-410b-4583-a306-a5b4f2378f01": [
            "f15188aa-76ab-4673-8419-485eccd306d2"
        ],
        "9580e8d1-8b62-4650-96c6-851eb640b670": [
            "9bdccab3-7157-479e-b784-fda1f8d22a1f"
        ],
        "8cdd5d6b-2170-40f5-8133-0959a4388cfa": [
            "9bdccab3-7157-479e-b784-fda1f8d22a1f"
        ],
        "c1481712-a0a0-47f3-9841-6805c3261aa0": [
            "71c22e84-2f31-4330-b398-ab9afb7b7c96"
        ],
        "11b04d53-1cc0-4cdb-aa98-f9e9d9b051fc": [
            "71c22e84-2f31-4330-b398-ab9afb7b7c96"
        ],
        "7f263c74-1c43-4da5-8066-155229e2d573": [
            "6d884f9d-84f6-48f0-a990-446e3e7adb47"
        ],
        "c0cbe4d5-2f6b-48c7-b7aa-e6f68d9abe20": [
            "6d884f9d-84f6-48f0-a990-446e3e7adb47"
        ],
        "c24c526c-1faf-4c0e-a0ba-d87f472071c3": [
            "832f04ff-aad1-46e5-8b20-8599a3d3eaed"
        ],
        "6a8ea374-42d6-42fe-917f-c24be2904022": [
            "832f04ff-aad1-46e5-8b20-8599a3d3eaed"
        ],
        "ba98261e-bf48-4a80-a404-8bfb309859bb": [
            "6b8552bc-27c9-4fae-b36b-ab95356d3ad9"
        ],
        "2144f0b3-53e9-4043-bd0b-9232960ea0a6": [
            "6b8552bc-27c9-4fae-b36b-ab95356d3ad9"
        ],
        "e02aa6ce-ccbf-46a8-a692-ed5e2ed906cb": [
            "1697c52c-2fe0-4d59-afeb-3cda27527d30"
        ],
        "24014f53-f936-4fce-b6d3-a859947839da": [
            "1697c52c-2fe0-4d59-afeb-3cda27527d30"
        ],
        "c89d7ac7-8b05-4cad-a0a1-c0926527aceb": [
            "d1206964-c1e5-4a32-81bc-b6299e2afead"
        ],
        "ae652eae-21eb-4136-aa68-94e1dd04f15d": [
            "d1206964-c1e5-4a32-81bc-b6299e2afead"
        ],
        "9a6f359f-b981-4bed-8d57-9959be93140e": [
            "3393c9c0-eb24-4dfa-981e-49d220dd70db"
        ],
        "ffc76758-45a8-41f0-9a46-8d55275804f9": [
            "3393c9c0-eb24-4dfa-981e-49d220dd70db"
        ],
        "cb697ae6-df09-4a9b-9e2e-530ab55991f3": [
            "994f6543-43ce-4f4e-a5f1-b497f3fc5abe"
        ],
        "db06343b-1325-424f-906c-82e362ed638f": [
            "994f6543-43ce-4f4e-a5f1-b497f3fc5abe"
        ],
        "845f06f7-45d3-4438-b0de-42b1d951485d": [
            "451aaefc-1c8c-49ef-8479-cbd2212a8c26"
        ],
        "6680415a-dd9b-45ce-b32f-6969c728f5bb": [
            "451aaefc-1c8c-49ef-8479-cbd2212a8c26"
        ],
        "68d698ef-fba3-4c5d-b5a6-0f9beb0942ad": [
            "540a4e06-0733-4982-a9e5-5d83c12c733b"
        ],
        "2b3564b4-a6b3-4024-aecd-5fd548b7ff72": [
            "540a4e06-0733-4982-a9e5-5d83c12c733b"
        ],
        "3616c951-fadf-4880-b15a-8e6cb6885f28": [
            "97c2dc10-d7a1-4455-8fb6-50a37a6e5277"
        ],
        "8cbf3829-9c8b-4f84-8b94-ef9e6405b3b6": [
            "97c2dc10-d7a1-4455-8fb6-50a37a6e5277"
        ],
        "a521d655-c505-467c-9e11-82577516fbca": [
            "6f426411-03d2-4f6f-9622-6da27b3f2b44"
        ],
        "04b069a3-e596-45eb-b96e-46a11862e0e6": [
            "6f426411-03d2-4f6f-9622-6da27b3f2b44"
        ],
        "7892c005-29d1-4f2d-9303-426a5d5fb10f": [
            "e604643f-6f16-45a2-87a8-f6a94e2de736"
        ],
        "0a6ad92b-2336-4c1f-8c60-119bc513fcdd": [
            "e604643f-6f16-45a2-87a8-f6a94e2de736"
        ],
        "19fe49d4-a74b-40ae-8111-1c5ec262876e": [
            "5c54d19d-0296-45c9-b55b-a6a316950429"
        ],
        "ee558096-0337-43e8-a77b-1c9dbde9dd65": [
            "5c54d19d-0296-45c9-b55b-a6a316950429"
        ],
        "2b6e5fce-a052-4fc4-8327-be7118456a1f": [
            "f85accf4-70f0-47bd-ab5f-76f767480415"
        ],
        "122c8f9a-aa3a-4a9c-b5ce-10a864419318": [
            "f85accf4-70f0-47bd-ab5f-76f767480415"
        ],
        "9adaa492-1a8e-40dc-b354-cf084907a235": [
            "191167c7-1e6f-4594-8914-2e85b56bee55"
        ],
        "21458fde-8f0d-4c29-8697-a63dddb1f097": [
            "191167c7-1e6f-4594-8914-2e85b56bee55"
        ],
        "a7c1d6d0-4149-4211-bd94-6cf51b9a8efe": [
            "2a24554d-cbc5-46ca-a5f8-f2f3090ab83d"
        ],
        "b6b44a65-8e85-4f90-b1cb-902c4ce67ada": [
            "2a24554d-cbc5-46ca-a5f8-f2f3090ab83d"
        ],
        "4b8a20b4-fe52-405b-97f1-b5bfa153b9b3": [
            "67e5d050-2aa2-43aa-935e-9feee7862100"
        ],
        "07a31cd9-09d4-415c-ae0f-9a9796ab7191": [
            "67e5d050-2aa2-43aa-935e-9feee7862100"
        ]
    },
    "mode": "text"
}