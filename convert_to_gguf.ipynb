{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate torch huggingface_hub \n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import create_repo, upload_file\n",
    "\n",
    "embedding_model_name = \"em_model\"\n",
    "qa_model_name = \"qa_model\"\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name, load_in_4bit=True)\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "\n",
    "qa_model = AutoModel.from_pretrained(qa_model_name, load_in_4bit=True)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "\n",
    "qa_model = qa_model.half().to(\"cuda\")\n",
    "\n",
    "save_directory_embedding = \"./quantized_em_model\"\n",
    "embedding_model.save_pretrained(save_directory_embedding)\n",
    "embedding_tokenizer.save_pretrained(save_directory_embedding)\n",
    "\n",
    "save_directory = \"./quantized_qa_model\"\n",
    "qa_model.save_pretrained(save_directory)\n",
    "qa_tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!pip install -r /kaggle/working/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt\n",
    "\n",
    "!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py /kaggle/working/quantized_qa_model --outfile qa.gguf --outtype auto\n",
    "!python /kaggle/working/llama.cpp/convert_hf_to_gguf.py /kaggle/working/quantized_em_model --outfile em.gguf --outtype auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"SurgicalMasks_Qwen2.5-3B-Instruct-4bit\"\n",
    "repo_name_embedding = \"SurgicalMasks_bge-large-en-v1.5\"\n",
    "create_repo(repo_name, token=\"TOKEN\")\n",
    "create_repo(repo_name_embedding, token=\"TOKEN\")\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj=\"qa.gguf\",\n",
    "    path_in_repo=\"model\",\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\",\n",
    "    token=\"TOKEN\"\n",
    ")\n",
    "upload_file(\n",
    "    path_or_fileobj=\"em.gguf\",\n",
    "    path_in_repo=\"model\",\n",
    "    repo_id=repo_name_embedding,\n",
    "    repo_type=\"model\",\n",
    "    token=\"TOKEN\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
